#!/bin/bash

# The SBATCH directives must appear before any executable line in this script.

#SBATCH --array=0-CHUNKS%1
#SBATCH --time=TIME               # Time: D-H:M:S
#SBATCH --account=rrg-keli              # Account: def-keli/rrg-keli
#SBATCH --mem=64G                       # Memory in total
#SBATCH --nodes=1                       # Number of nodes requested.
#SBATCH --cpus-per-task=10              # Number of cores per task.
#SBATCH --gres=gpu:a100:NUM_GPUS        # 40G A100

#SBATCH --job-name=NAME
#SBATCH --output=job_results/_%x_%a.txt

# Below sets the email notification, swap to your email to receive notifications
#SBATCH --mail-user=shinminje@gmail.com
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --mail-type=REQUEUE
#SBATCH --mail-type=ALL

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
# Print some info for context.
pwd
hostname
date

module load httpproxy

echo "Starting job number $SLURM_ARRAY_TASK_ID"

source ~/.bashrc
conda activate py39ISICLE

# Python will buffer output of your script unless you set this.
export PYTHONUNBUFFERED=1

# Delete the old file containing all the results if we're starting running a new array.
if [[ $SLURM_ARRAY_TASK_ID == 0 ]]
then
	rm job_results/NAME_all_results.txt
fi

SCRIPT

# Print completion time.
date

# Add the results of the current task to the file with the results of all tasks.
echo "----- START OF TASK $SLURM_ARRAY_TASK_ID -----" >> job_results/NAME_all_results.txt
cat job_results/_NAME_$SLURM_ARRAY_TASK_ID.txt >> job_results/NAME_all_results.txt
